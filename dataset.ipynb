{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44db97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pytz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105bf5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Dataset/cleaned_texas_data.csv')\n",
    "\n",
    "#Rename a column\n",
    "df=df.rename(columns={'san antonio': 'san_antonio'})\n",
    "\n",
    "df.to_csv('Dataset/cleaned_texas_data.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46387daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your CSV file\n",
    "df=pd.read_csv('Dataset/cleaned_subregion_data.csv')\n",
    "\n",
    "#Replace spaces with underscores in the 'city' column\n",
    "df['city']=df['city'].str.replace(' ','_')\n",
    "\n",
    "#(Optional) Save the updated dataframe to a new CSV file\n",
    "df.to_csv('Dataset/cleaned_subregion_data.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d3011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'combined.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "folder='Dataset' \n",
    "alldata=[]\n",
    "\n",
    "#Loop through each JSON file\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith('.json'):\n",
    "        cityname=os.path.splitext(filename)[0]  #Remove .json extension\n",
    "        filepath=os.path.join(folder,filename)\n",
    "        \n",
    "        with open(filepath,'r') as f:\n",
    "            records=json.load(f)  #This is a list of dictionaries\n",
    "            for record in records:\n",
    "                record['city']=cityname  #Add city to each record\n",
    "                alldata.append(record)\n",
    "\n",
    "#Convert to DataFrame\n",
    "df=pd.DataFrame(alldata)\n",
    "\n",
    "#Save to CSV\n",
    "df.to_csv('combined.csv',index=False)\n",
    "print(\"CSV file 'combined.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73898dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV with time conversion saved as 'finalcombined.csv'.\n"
     ]
    }
   ],
   "source": [
    "#Load the combined CSV\n",
    "df=pd.read_csv('combined.csv')\n",
    "\n",
    "#Convert UNIX timestamp to datetime\n",
    "df['utc_time']=pd.to_datetime(df['time'],unit='s',utc=True)\n",
    "\n",
    "citytimezonemap = {\n",
    "    'dallas':'America/Chicago',\n",
    "    'nyc':'America/New_York',\n",
    "    'houston':'America/Chicago',\n",
    "    'la':'America/Los_Angeles',\n",
    "    'philadelphia':'America/New_York',\n",
    "    'phoenix':'America/Phoenix',\n",
    "    'san_antonio':'America/Chicago',\n",
    "    'san_diego':'America/Los_Angeles',\n",
    "    'san_jose':'America/Los_Angeles',\n",
    "    'seattle':'America/Los_Angeles'\n",
    "}\n",
    "\n",
    "#Convert to local time based on city\n",
    "def converttolocal(row):\n",
    "    tz_name=citytimezonemap.get(row['city'].lower(),'UTC')\n",
    "    local_tz=pytz.timezone(tz_name)\n",
    "    return row['utc_time'].astimezone(local_tz)\n",
    "\n",
    "df['local_time']=df.apply(converttolocal,axis=1)\n",
    "\n",
    "#Keep only the requested columns\n",
    "dffiltered=df[['local_time','utc_time','city','temperature','humidity','windSpeed']]\n",
    "\n",
    "#Save to new CSV\n",
    "dffiltered.to_csv('finalcombined.csv',index=False)\n",
    "print(\"Final CSV with time conversion saved as 'finalcombined.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1499d003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riyya\\AppData\\Local\\Temp\\ipykernel_1688\\336661894.py:101: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('finaldataset.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved as 'finaldataset.csv'\n"
     ]
    }
   ],
   "source": [
    "#PART 1:Merge weather data with first two demand datasets\n",
    "\n",
    "#Load weather dataset\n",
    "\n",
    "df=pd.read_csv('finalcombined.csv')\n",
    "\n",
    "#Convert 'utc_time' to datetime and remove timezone info for consistency\n",
    "df['utc_time']=pd.to_datetime(df['utc_time']).dt.tz_localize(None)\n",
    "df['city']=df['city'].str.lower()\n",
    "\n",
    "#Load and clean first demand dataset (balance data)\n",
    "\n",
    "balancedf=pd.read_csv('Dataset/cleaned_balance_data.csv')\n",
    "balancedf['utc_time']=pd.to_datetime(balancedf['utc_time']).dt.tz_localize(None)\n",
    "balancedf['city']=balancedf['city'].str.lower()\n",
    "balancedf=balancedf[['city','utc_time','demand']]\n",
    "\n",
    "#Load and clean second demand dataset (subregion data)\n",
    "\n",
    "subregiondf=pd.read_csv('Dataset/cleaned_subregion_data.csv')\n",
    "subregiondf['utc_time']=pd.to_datetime(subregiondf['utc_time']).dt.tz_localize(None)\n",
    "subregiondf['city']=subregiondf['city'].str.lower()\n",
    "subregiondf=subregiondf[['city','utc_time','demand']]\n",
    "\n",
    "#Combine first two demand datasets\n",
    "combineddemand=pd.concat([balancedf,subregiondf],ignore_index=True)\n",
    "\n",
    "#Merge weather data with combined demand data\n",
    "weatherwithdemand=pd.merge(\n",
    "    df,\n",
    "    combineddemand,\n",
    "    on=['city','utc_time'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#PART 2:Add the wide-format demand data\n",
    "\n",
    "widedemanddf=pd.read_csv('Dataset/cleaned_texas_data.csv')\n",
    "\n",
    "#Convert from wide to long format\n",
    "longdemanddf=pd.melt(\n",
    "    widedemanddf,\n",
    "    id_vars=['date'],\n",
    "    value_vars=['houston','san_antonio','dallas'],\n",
    "    var_name='city',\n",
    "    value_name='demand'\n",
    ")\n",
    "\n",
    "#Convert date to datetime and use it as local_time\n",
    "longdemanddf['local_time']=pd.to_datetime(longdemanddf['date']).dt.tz_localize(None)\n",
    "longdemanddf=longdemanddf.drop('date',axis=1)\n",
    "\n",
    "#Convert local_time to utc_time based on city timezone\n",
    "#Define timezone offsets (hours from UTC)\n",
    "timezoneoffsets={\n",
    "    'houston':-6, #Central Time\n",
    "    'dallas':-6,  #Central Time\n",
    "    'san_antonio':-6  #Central Time\n",
    "}\n",
    "\n",
    "#Convert local time to UTC based on city timezone\n",
    "def localtoutc(row):\n",
    "    offset=timezoneoffsets.get(row['city'],-6)  #Default to Central Time\n",
    "    return row['local_time'] + pd.Timedelta(hours=offset)\n",
    "\n",
    "#Apply the conversion\n",
    "longdemanddf['utc_time']=longdemanddf.apply(localtoutc,axis=1)\n",
    "longdemanddf['city']=longdemanddf['city'].str.lower()\n",
    "#print(f\"Transformed wide format data shape:{longdemanddf.shape}\")\n",
    "\n",
    "#Merge with the existing weather and demand data\n",
    "#First,preserve existing demand values\n",
    "weatherwithdemand['demand_original']=weatherwithdemand['demand']\n",
    "\n",
    "#Perform the merge\n",
    "finalmergeddf=pd.merge(\n",
    "    weatherwithdemand,\n",
    "    longdemanddf,\n",
    "    on=['city','utc_time'],\n",
    "    how='left',\n",
    "    suffixes=('','_new')\n",
    ")\n",
    "\n",
    "#Handle demand values (prioritize new values where available)\n",
    "#Where new demand exists,use it; otherwise keep original\n",
    "finalmergeddf['demand']=finalmergeddf['demand_new'].fillna(finalmergeddf['demand_original'])\n",
    "\n",
    "#Clean up intermediate columns\n",
    "finalmergeddf=finalmergeddf.drop(['demand_new','demand_original'],axis=1)\n",
    "\n",
    "#PART 3:Clean and save the final dataset\n",
    "\n",
    "#Sort by city and time for readability\n",
    "finalmergeddf=finalmergeddf.sort_values(['city','utc_time'])\n",
    "\n",
    "#Create a cleaned version with no missing demand values\n",
    "cleaneddf=finalmergeddf.dropna(subset=['demand'])\n",
    "cleaneddf.to_csv('finaldataset.csv',index=False)\n",
    "\n",
    "#Load the CSV file\n",
    "df=pd.read_csv('finaldataset.csv')\n",
    "\n",
    "#Drop a column (e.g.,'city_code')\n",
    "df=df.drop(columns=['local_time_new'])\n",
    "\n",
    "#Save the updated DataFrame back to CSV\n",
    "df.to_csv('finaldataset.csv',index=False)\n",
    "print(\"✅ Cleaned dataset saved as 'finaldataset.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6878b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Daily Summary Statistics ---\n",
      "  local_time temperature                      humidity windSpeed     demand  \\\n",
      "                    mean       min       max      mean      mean        sum   \n",
      "0 2018-07-01    0.466620 -0.353181  2.362830 -0.235490 -0.418424 -25.841166   \n",
      "1 2018-07-02    0.924849 -0.612149  2.618640 -0.263539 -0.212833  80.988616   \n",
      "2 2018-07-03    0.993066 -0.783321  2.690015 -0.238172 -0.129051  96.370379   \n",
      "3 2018-07-04    0.904790 -0.459295  2.633800 -0.100643 -0.191089  40.606722   \n",
      "4 2018-07-05    0.960318 -0.326021  3.062677 -0.007361 -0.261514  70.559797   \n",
      "\n",
      "                       \n",
      "       mean       max  \n",
      "0 -0.323015  1.520864  \n",
      "1  0.373219  3.608134  \n",
      "2  0.401543  3.607970  \n",
      "3  0.169195  2.806371  \n",
      "4  0.293999  3.266932  \n",
      "\n",
      "--- Weekly Summary Statistics ---\n",
      "  local_time temperature                      humidity windSpeed      demand  \\\n",
      "                    mean       min       max      mean      mean         sum   \n",
      "0 2018-07-01    0.466620 -0.353181  2.362830 -0.235490 -0.418424  -25.841166   \n",
      "1 2018-07-08    1.012567 -0.783321  3.092363 -0.285717 -0.159626  518.684630   \n",
      "2 2018-07-15    1.003392 -0.371498  2.746861 -0.130076 -0.149876  706.899885   \n",
      "3 2018-07-22    1.093002 -0.651311  2.923718 -0.240419  0.076264  847.758060   \n",
      "4 2018-07-29    1.235267 -0.257805  3.221848 -0.448220 -0.148647  917.776432   \n",
      "\n",
      "                       \n",
      "       mean       max  \n",
      "0 -0.323015  1.520864  \n",
      "1  0.313026  3.608134  \n",
      "2  0.465985  3.398514  \n",
      "3  0.579070  4.042792  \n",
      "4  0.613077  3.853907  \n",
      "\n",
      "--- Z-Score Based Demand Anomalies ---\n",
      "           local_time    demand  demand_z\n",
      "3 2018-07-02 08:00:00  3.199500  3.199500\n",
      "4 2018-07-02 09:00:00  3.406367  3.406367\n",
      "5 2018-07-02 10:00:00  3.536530  3.536530\n",
      "6 2018-07-02 11:00:00  3.597606  3.597606\n",
      "7 2018-07-02 12:00:00  3.608134  3.608134\n",
      "\n",
      "--- Isolation Forest Detected Anomalies ---\n",
      "             local_time  temperature  humidity  windSpeed    demand\n",
      "372 2018-07-17 18:00:00     2.302194 -1.485891   0.723353  2.508474\n",
      "376 2018-07-17 22:00:00     2.564952 -1.945597   0.474962  1.499655\n",
      "392 2018-07-18 15:00:00     1.630770 -0.566479   0.914642  3.391139\n",
      "393 2018-07-18 16:00:00     1.885317 -0.934244   0.751904  3.195454\n",
      "394 2018-07-18 17:00:00     2.204291 -1.302009   0.406440  2.863553\n"
     ]
    }
   ],
   "source": [
    "#Load CSV without date parsing first\n",
    "df=pd.read_csv(\"finaldataset.csv\")\n",
    "\n",
    "#Convert local_time to datetime and remove timezone\n",
    "df[\"local_time\"]=pd.to_datetime(df[\"local_time\"],utc=True).dt.tz_localize(None)\n",
    "\n",
    "#Now safe to use .dt accessors\n",
    "df[\"hour\"]=df[\"local_time\"].dt.hour\n",
    "df[\"dayofweek\"]=df[\"local_time\"].dt.dayofweek\n",
    "df[\"month\"]=df[\"local_time\"].dt.month\n",
    "\n",
    "\n",
    "#Define seasons (Northern Hemisphere)\n",
    "def get_season(month):\n",
    "    if month in [12,1,2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3,4,5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6,7,8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df[\"season\"]=df[\"month\"].apply(get_season)\n",
    "\n",
    "#Normalize continuous features\n",
    "scaler=StandardScaler()\n",
    "df[[\"temperature\",\"humidity\",\"windSpeed\",\"demand\"]]=scaler.fit_transform(\n",
    "    df[[\"temperature\",\"humidity\",\"windSpeed\",\"demand\"]])\n",
    "\n",
    "#Aggregation\n",
    "\n",
    "#Daily summary stats\n",
    "dailysummary=df.resample('D',on='local_time').agg({\n",
    "    \"temperature\": [\"mean\",\"min\",\"max\"],\n",
    "    \"humidity\": [\"mean\"],\n",
    "    \"windSpeed\": [\"mean\"],\n",
    "    \"demand\": [\"sum\",\"mean\",\"max\"]\n",
    "}).reset_index()\n",
    "\n",
    "#Weekly summary stats\n",
    "weeklysummary=df.resample('W',on='local_time').agg({\n",
    "    \"temperature\": [\"mean\",\"min\",\"max\"],\n",
    "    \"humidity\": [\"mean\"],\n",
    "    \"windSpeed\": [\"mean\"],\n",
    "    \"demand\": [\"sum\",\"mean\",\"max\"]\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "print(\"\\n--- Daily Summary Statistics ---\")\n",
    "print(dailysummary.head())\n",
    "\n",
    "print(\"\\n--- Weekly Summary Statistics ---\")\n",
    "print(weeklysummary.head())\n",
    "\n",
    "#Anomaly & Error Detection\n",
    "\n",
    "#Z-Score method\n",
    "df[\"demand_z\"]=stats.zscore(df[\"demand\"])\n",
    "zanomalies=df[np.abs(df[\"demand_z\"]) > 3]\n",
    "\n",
    "#Isolation Forest\n",
    "features=scaler.fit_transform(df[[\"temperature\",\"humidity\",\"windSpeed\",\"demand\"]])\n",
    "isoforest=IsolationForest(contamination=0.01,random_state=42)\n",
    "df[\"iforest_flag\"]=isoforest.fit_predict(features)\n",
    "iforestanomalies=df[df[\"iforest_flag\"] == -1]\n",
    "\n",
    "#Display\n",
    "print(\"\\n--- Z-Score Based Demand Anomalies ---\")\n",
    "print(zanomalies[[\"local_time\",\"demand\",\"demand_z\"]].head())\n",
    "\n",
    "print(\"\\n--- Isolation Forest Detected Anomalies ---\")\n",
    "print(iforestanomalies[[\"local_time\",\"temperature\",\"humidity\",\"windSpeed\",\"demand\"]].head())\n",
    "\n",
    "\n",
    "#Save processed data\n",
    "\n",
    "\n",
    "df=df.drop('iforest_flag',axis=1)\n",
    "df=df.drop('demand_z',axis=1)\n",
    "#Drop rows with any missing values\n",
    "df=df.dropna(subset=['temperature','humidity','windSpeed','demand'])\n",
    "\n",
    "df.to_csv(\"scaleddataset.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
